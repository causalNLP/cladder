{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsZIlg5NWttO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from google.colab import files\n",
    "from google.colab import drive\n",
    "import gspread\n",
    "from google.auth import default"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone https://github.com/felixludos/omni-data\n",
    "!git clone https://github.com/felixludos/omni-belt\n",
    "!pip install omni-belt/. omni-data/. omnifig\n",
    "!apt-get -qq install -y graphviz && pip install pydot"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2EkbUblSnHxU",
    "outputId": "d825c24f-ae65-4844-c493-ad30fdaa750d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'omni-data'...\n",
      "remote: Enumerating objects: 1890, done.\u001B[K\n",
      "remote: Counting objects: 100% (807/807), done.\u001B[K\n",
      "remote: Compressing objects: 100% (276/276), done.\u001B[K\n",
      "remote: Total 1890 (delta 571), reused 745 (delta 524), pack-reused 1083\u001B[K\n",
      "Receiving objects: 100% (1890/1890), 1.26 MiB | 2.35 MiB/s, done.\n",
      "Resolving deltas: 100% (1339/1339), done.\n",
      "Cloning into 'omni-belt'...\n",
      "remote: Enumerating objects: 1137, done.\u001B[K\n",
      "remote: Counting objects: 100% (4/4), done.\u001B[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001B[K\n",
      "remote: Total 1137 (delta 0), reused 1 (delta 0), pack-reused 1133\u001B[K\n",
      "Receiving objects: 100% (1137/1137), 6.83 MiB | 11.17 MiB/s, done.\n",
      "Resolving deltas: 100% (713/713), done.\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Processing ./omni-belt\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Processing ./omni-data\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting omnifig\n",
      "  Downloading omnifig-1.0.0.tar.gz (57 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 KB\u001B[0m \u001B[31m811.6 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from omnibelt==0.7.3) (6.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m110.5/110.5 KB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hCollecting wrapt==1.11.2\n",
      "  Downloading wrapt-1.11.2.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting cryptography==2.7\n",
      "  Downloading cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.3/2.3 MB\u001B[0m \u001B[31m35.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.9/dist-packages (from omnibelt==0.7.3) (1.22.4)\n",
      "Collecting indexed==1.3.0\n",
      "  Downloading indexed-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Collecting asn1crypto>=0.21.0\n",
      "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m105.0/105.0 KB\u001B[0m \u001B[31m8.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25hRequirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.9/dist-packages (from cryptography==2.7->omnibelt==0.7.3) (1.15.1)\n",
      "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from cryptography==2.7->omnibelt==0.7.3) (1.16.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from omnifig) (0.10.2)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from omnifig) (0.8.10)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi!=1.11.3,>=1.8->cryptography==2.7->omnibelt==0.7.3) (2.21)\n",
      "Building wheels for collected packages: omnibelt, wrapt, omni-data, omnifig\n",
      "  Building wheel for omnibelt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for omnibelt: filename=omnibelt-0.7.3-py3-none-any.whl size=73974 sha256=508d29bf386658576de1f56f34897a42cc227b806ff86669963df2ad442a058f\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/11/a9/f8d843d9776775fde747802333bd29c1a553d6912595bc3b7e\n",
      "  Building wheel for wrapt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.2-cp39-cp39-linux_x86_64.whl size=75721 sha256=f7a6487830a13821f79b8349be3685b875a45f474ea2af1b5318144a7572f3f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/76/e9/66/d4e35bfa6cde3925ff1c497043d7a2ccb305c07ac51fef0e31\n",
      "  Building wheel for omni-data (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for omni-data: filename=omni_data-0.1-py3-none-any.whl size=87377 sha256=7a605e9017fe79fdc9ebf911233d88588a73ef599f2e38ea341ee994aa49cc0b\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/89/de/3f8cbb7fdd74a9411197c34ee46eced317949ff452c067ebf9\n",
      "  Building wheel for omnifig (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for omnifig: filename=omnifig-1.0.0-py3-none-any.whl size=62540 sha256=a163fcd296b481fd5fbd376cb10dedb41ae66a7648c2b83f0e6be064c699e3b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/e1/9a/87/1bfd4396a81917a92b0aad09c2fc0a965e1118867962365535\n",
      "Successfully built omnibelt wrapt omni-data omnifig\n",
      "Installing collected packages: wrapt, omni-data, indexed, asn1crypto, dill, cryptography, omnibelt, omnifig\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.14.1\n",
      "    Uninstalling wrapt-1.14.1:\n",
      "      Successfully uninstalled wrapt-1.14.1\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 40.0.1\n",
      "    Uninstalling cryptography-40.0.1:\n",
      "      Successfully uninstalled cryptography-40.0.1\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yfinance 0.2.14 requires cryptography>=3.3.2, but you have cryptography 2.7 which is incompatible.\u001B[0m\u001B[31m\n",
      "\u001B[0mSuccessfully installed asn1crypto-1.5.1 cryptography-2.7 dill-0.3.6 indexed-1.3.0 omni-data-0.1 omnibelt-0.7.3 omnifig-1.0.0 wrapt-1.11.2\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.9/dist-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.9/dist-packages (from pydot) (3.0.9)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from omnidata import hparam, inherit_hparams, submodule, submachine, spaces\n",
    "from omnidata import Guru, Context, material, space, indicator, machine, Structured"
   ],
   "metadata": {
    "id": "9JNtZhYznLkG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ],
   "metadata": {
    "id": "uoXOZxNny3rW",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "31166954-36dc-4606-fa66-db633d6d334d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fatal Python error: config_init_hash_seed: PYTHONHASHSEED must be \"random\" or an integer in range [0; 4294967295]\n",
      "Python runtime state: preinitialized\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "creds, _ = default()\n",
    "gc = gspread.authorize(creds)"
   ],
   "metadata": {
    "id": "iki8F1C9y63Y"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#defining my worksheet\n",
    "graph2story = gc.open('20230212_Elements_for_Data_Generation').worksheet('graph2story_finer')\n",
    "graph2temp = gc.open('20230212_Elements_for_Data_Generation').worksheet('graph2temp_finer')\n",
    "graph2query = gc.open('20230212_Elements_for_Data_Generation').worksheet('query_dependencies')\n",
    "\n",
    "#get_all_values gives a list of rows\n",
    "dat1 = pd.DataFrame(graph2story.get_all_values())\n",
    "dat2 = pd.DataFrame(graph2temp.get_all_values())\n",
    "dat_query = pd.DataFrame(graph2query.get_all_values())\n",
    "#Convert to a DataFrame \n",
    "dat1.columns = dat1.iloc[1]\n",
    "dat2.columns = dat2.iloc[0]\n",
    "dat_query.columns = dat_query.iloc[1]\n",
    "\n",
    "# Remove the first two rows and reset the index\n",
    "dat1 = dat1.iloc[2:].reset_index(drop=True)\n",
    "dat2 = dat2.iloc[1:].reset_index(drop=True)\n",
    "dat_query = dat_query.iloc[2:].reset_index(drop=True)\n",
    "\n",
    "# Drop the empty column\n",
    "dat1 = dat1.drop(columns=[''])\n",
    "dat1.columns.name = None\n",
    "dat2.columns.name = None\n",
    "dat_query.columns.name = None\n",
    "# Print the DataFrame\n"
   ],
   "metadata": {
    "id": "CAlMJp_onVhT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "merged_df = dat1.merge(dat2, on='causal_graph_type', how='left')\n",
    "dat = merged_df.drop(columns = ['semantic_graph (by topological order)_y','variable_correspondence_y'])\n",
    "\n",
    "dat['relation1'] = dat['relation1'].str.split(', ')\n",
    "dat['relation2'] = dat['relation2'].str.split(', ')\n",
    "\n",
    "dat.fillna('', inplace = True)"
   ],
   "metadata": {
    "id": "hDCfhmlrEcKv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "dat_query['info_type'] = dat_query['info_type'].str.split(', ')\n",
    "dat_query['causal_graph_type'] = dat_query['causal_graph_type'].str.split(', ')\n",
    "dat_query.fillna('', inplace = True)"
   ],
   "metadata": {
    "id": "PAFNxtiY0ESj"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# diamond only\n",
    "\n",
    "class Current(Structured): # Create a class for the current SCM\n",
    "    binary_op = hparam('and')\n",
    "    @material.from_size('X')\n",
    "    def generate_X(self, N = 1):\n",
    "    return np.random.choice([True, False], p=[0.5, 0.5]) # Bernoulli(0.5) prior\n",
    "\n",
    "    @machine('Z')\n",
    "    def Z(self, X):\n",
    "    return X\n",
    "\n",
    "    @machine('W')\n",
    "    def W(self, X):\n",
    "    return X\n",
    "\n",
    "    @machine('Y')\n",
    "    def Y(self, Z, W):\n",
    "        return (Z and W) if self.binary_op == 'and' else (Z or W)\n",
    "\n",
    "scms = {}\n",
    "\n",
    "scms['firing_squad', 'diamond'] = Current(binary_op='or', application={'X':'captain', 'W': 'rifleman1', 'Z': 'rifleman2', 'Y': 'prisoner'})\n",
    "scms['sprinkler', 'diamond'] = Current(binary_op='or', application={'X':'season', 'W': 'rain', 'Z': 'sprinkler', 'Y': 'wet_floor'})\n",
    "\n",
    "with open('prop_diamond.json', 'w') as out_file:\n",
    "  out_file.write(\"[\\n\")\n",
    "  # Counter for number of entries written\n",
    "  count = 0\n",
    "  for idx, row in dat.iterrows():\n",
    "    if row['causal_graph_type'] == 'diamond':\n",
    "\n",
    "      # loop over 2 different Structural equations\n",
    "      for rel1 in row['relation1']:\n",
    "\n",
    "        # class Current(Structured): # Create a class for the current SCM\n",
    "        #   @material.from_size('X')\n",
    "        #   def generate_X(self, N = 1):\n",
    "        #     return np.random.choice([True, False], p=[0.5, 0.5]) # Bernoulli(0.5) prior\n",
    "\n",
    "        #   @machine('Z')\n",
    "        #   def Z(self, X):\n",
    "        #     return X\n",
    "\n",
    "        #   @machine('W')\n",
    "        #   def W(self, X):\n",
    "        #     return X\n",
    "\n",
    "        #   @machine('Y')\n",
    "        #   def Y(self, Z, W):\n",
    "        #     if rel1 == 'and':\n",
    "        #       return Z and W\n",
    "        #     else:\n",
    "        #       return Z or W\n",
    "\n",
    "        # Instantiate the SCM\n",
    "        ctx = Guru(Current(binary_op=rel1))\n",
    "        s = {'X','Z','W','Y'}\n",
    "        powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "        powerset = [set(subset) for subset in powerset]\n",
    "\n",
    "        # loop over all sets of observed variables\n",
    "        for obs_var in powerset:\n",
    "          n = len(obs_var)\n",
    "          # if nothing observed(n == 0) -> ask interventional questions only (do_calculus)\n",
    "          if n == 0: \n",
    "            filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'], W_string=row['W_string_name'], \n",
    "                                                                      X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                      W_0 = row['W_0'], W_1 = row['W_1'], rel1 = rel1)\n",
    "            \n",
    "            # loop over all applicible queries (do_calculus only)\n",
    "            for index, question in dat_query.iterrows():\n",
    "              if row['causal_graph_type'] in question['causal_graph_type'] and row['info_type'] in question['info_type'] and question['CI_engine to get the answer'] == 'do_calculus':\n",
    "                s2 = {'X','Z','W'}\n",
    "                powerset2 = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s2, r) for r in range(len(s2)+1)))\n",
    "                powerset2 = [set(subset) for subset in powerset2]\n",
    "                for intervention_set in powerset2:\n",
    "                  if len(intervention_set) > 0:\n",
    "                    # loop over 2^n possible n-tuples\n",
    "                    for intervention_val in list(itertools.product(*[(0, 1)] * len(intervention_set))):\n",
    "                      interv = {}\n",
    "                      interv_text = \"\"\n",
    "                      # for each n-tuple, create a dictionary of interventions\n",
    "                      for i, (k, v) in enumerate(zip(intervention_set, intervention_val)):\n",
    "\n",
    "                        interv_text += row[f'{k}_{v}']\n",
    "                        interv[k] = v\n",
    "                        if i != len(intervention_set) - 1:\n",
    "                            interv_text += ', '\n",
    "\n",
    "                    for y in [0,1]:\n",
    "                      filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'], W_string=row['W_string_name'], \n",
    "                                                                              X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                              W_0 = row['W_0'], W_1 = row['W_1'], rel1 = rel1)\n",
    "                      query = question['query_word_form'].format(Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                            X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                            W_0 = row['W_0'], W_1 = row['W_1'], intervention = interv_text, Y_int = row[f'Y_{y}'])\n",
    "\n",
    "                      # Perform interventions\n",
    "                      for (var, val) in interv.items():\n",
    "                        ctx[var] = val\n",
    "                      \n",
    "                      result = int(ctx['Y'] == y)\n",
    "                      y_truth = int(ctx['Y'])\n",
    "                      result_text = row[f'Y_{y_truth}']\n",
    "                      target = 'Yes' if result else 'No'\n",
    "                      \n",
    "\n",
    "                      ctx.clear()\n",
    "              \n",
    "                      if count > 0:\n",
    "                        out_file.write(\",\\n\")\n",
    "                            \n",
    "                      data = {\n",
    "                            \"causal_graph_type\": row['causal_graph_type'],\n",
    "                            \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                            \"given_info_type\": row['info_type'],\n",
    "                            \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                            \"target\": target,\n",
    "                            \"truth\": result_text\n",
    "                        }\n",
    "                      # Write data to file\n",
    "                      json.dump(data, out_file, indent=4)\n",
    "                      # Increment counter\n",
    "                      count += 1\n",
    "\n",
    "          # With observations, answer counterfactual questions.\n",
    "          else:\n",
    "            for obs_val in list(itertools.product(*[(0, 1)] * (n))):\n",
    "              evid_text = \"\"\n",
    "              evid = {}\n",
    "              for i, (k, v) in enumerate(zip(obs_var, obs_val)):\n",
    "                evid_text += row[f'{k}_{v}']\n",
    "                evid[k] = v\n",
    "                if i == len(obs_var) - 1:\n",
    "                    evid_text += '.'\n",
    "                else:\n",
    "                    evid_text += ', '\n",
    "              filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'], W_string=row['W_string_name'], \n",
    "                                                                      X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                      W_0 = row['W_0'], W_1 = row['W_1'], rel1 = rel1) +' '+\\\n",
    "              row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'], W_string=row['W_string_name'],X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                      W_0 = row['W_0'], W_1 = row['W_1'], rel1 = rel1, evidence = evid_text)\n",
    "              for index, question in dat_query.iterrows():\n",
    "                if row['causal_graph_type'] in question['causal_graph_type'] and row['info_type'] in question['info_type'] and question['CI_engine to get the answer'] == 'counterfactual':\n",
    "                  for (cou_var, cou_val, y) in zip(['X','Z','W'],[0,1],[0,1]):\n",
    "                    if cou_var not in evid or cou_val != evid[cou_var]:\n",
    "                      query = question['query_word_form'].format(Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'],\n",
    "                                                                X_string = row['X_string_name'], X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                W_0 = row['W_0'], W_1 = row['W_1'], cou_var = row[f'{cou_var}_string_name'], cou_val = row[f'{cou_var}_{cou_val}'], Y_int = row[f'Y_{y}'], cou_val_original = row[f'{cou_var}_{abs(1-cou_val)}'])\n",
    "                      \n",
    "                      # Input observations into the SCM\n",
    "                      for (var, val) in evid.items():\n",
    "                        ctx[var] = val\n",
    "\n",
    "                      # Input counterfactual by setting cou_var to cou_val\n",
    "                      ctx[cou_var] = bool(cou_val)\n",
    "                      result = int(ctx['Y'] == y)\n",
    "\n",
    "                      # True state of Y in counterfactual world\n",
    "                      y_truth = int(ctx['Y'])\n",
    "                      result_text = row[f'Y_{y_truth}']\n",
    "                      target = 'Yes' if result else 'No'\n",
    "                      \n",
    "\n",
    "                      ctx.clear()\n",
    "                \n",
    "                      if count > 0:\n",
    "                        out_file.write(\",\\n\")\n",
    "                            \n",
    "                      data = {\n",
    "                            \"causal_graph_type\": row['causal_graph_type'],\n",
    "                            \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                            \"given_info_type\": row['info_type'],\n",
    "                            \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                            \"target\": target,\n",
    "                            \"truth\": result_text\n",
    "                        }\n",
    "                      # Write data to file\n",
    "                      json.dump(data, out_file, indent=4)\n",
    "                      # Increment counter\n",
    "                      count += 1\n",
    "\n",
    "                \n",
    "              \n",
    "  out_file.write(\"\\n]\")\n",
    "\n",
    "out_file.close()"
   ],
   "metadata": {
    "id": "AJj6Focpc_Ei",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "outputId": "59577091-0943-41cf-d76b-2aa5f79f1756"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;36m  File \u001B[0;32m\"<ipython-input-1-ebe1f68db78d>\"\u001B[0;36m, line \u001B[0;32m7\u001B[0m\n\u001B[0;31m    return np.random.choice([True, False], p=[0.5, 0.5]) # Bernoulli(0.5) prior\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mIndentationError\u001B[0m\u001B[0;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# fork graph\n",
    "with open('prop_fork.json', 'w') as out_file:\n",
    "  out_file.write(\"[\\n\")\n",
    "  # Counter for number of entries written\n",
    "  count = 0\n",
    "  for idx, row in dat.iterrows():\n",
    "    if row['causal_graph_type'] == '>':\n",
    "      # loop over 2 different Structural equations\n",
    "      for rel1 in row['relation1']:\n",
    "        class Current(Structured): # Create a class for the current SCM\n",
    "          @material.from_size('X')\n",
    "          def generate_X(self, N = 1):\n",
    "            return np.random.choice([True, False], p=[0.5, 0.5]) # Bernoulli(0.5) prior\n",
    "\n",
    "          @machine('Z')\n",
    "          def Z(self, X):\n",
    "            return X\n",
    "\n",
    "          @machine('Y')\n",
    "          def Y(self, X, Z):\n",
    "            if rel1 == 'and':\n",
    "              return X and Z\n",
    "            else:\n",
    "              return X or Z\n",
    "\n",
    "        # Instantiate the SCM\n",
    "        ctx = Guru(Current())\n",
    "        s = {'X','Z','Y'}\n",
    "        powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "        powerset = [set(subset) for subset in powerset]\n",
    "\n",
    "        # loop over all sets of observed variables\n",
    "        for obs_var in powerset:\n",
    "          n = len(obs_var)\n",
    "          # if nothing observed(n == 0) -> ask interventional questions only (do_calculus)\n",
    "          if n == 0: \n",
    "            filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                                     X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                     rel1 = rel1)\n",
    "            \n",
    "            # loop over all applicible queries (do_calculus only)\n",
    "            for index, question in dat_query.iterrows():\n",
    "              if row['causal_graph_type'] in question['causal_graph_type'] and row['info_type'] in question['info_type'] and question['CI_engine to get the answer'] == 'do_calculus':\n",
    "                s2 = {'X','Z'}      \n",
    "                powerset2 = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s2, r) for r in range(len(s2)+1)))\n",
    "                powerset2 = [set(subset) for subset in powerset2]\n",
    "                for intervention_set in powerset:\n",
    "                  if len(intervention_set) > 0:\n",
    "                    # loop over 2^n possible n-tuples\n",
    "                    for intervention_val in list(itertools.product(*[(0, 1)] * len(intervention_set))):\n",
    "                      interv = {}\n",
    "                      interv_text = \"\"\n",
    "                      # for each n-tuple, create a dictionary of interventions\n",
    "                      for i, (k, v) in enumerate(zip(intervention_set, intervention_val)):\n",
    "\n",
    "                        interv_text += row[f'{k}_{v}']\n",
    "                        interv[k] = v\n",
    "                        if i != len(intervention_set) - 1:\n",
    "                            interv_text += ', '\n",
    "\n",
    "                      for y in [0,1]:\n",
    "                        filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'], W_string=row['W_string_name'], \n",
    "                                                                                X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                                W_0 = row['W_0'], W_1 = row['W_1'], rel1 = rel1)\n",
    "                        query = question['query_word_form'].format(Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                              X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                              W_0 = row['W_0'], W_1 = row['W_1'], intervention = interv_text, Y_int = row[f'Y_{y}'])\n",
    "\n",
    "                        # Perform interventions\n",
    "                        for (var, val) in interv.items():\n",
    "                          ctx[var] = val\n",
    "                        \n",
    "                        result = int(ctx['Y'] == y)\n",
    "                        y_truth = int(ctx['Y'])\n",
    "                        result_text = row[f'Y_{y_truth}']\n",
    "                        target = 'Yes' if result else 'No'\n",
    "                        \n",
    "\n",
    "                        ctx.clear()\n",
    "                \n",
    "                        if count > 0:\n",
    "                          out_file.write(\",\\n\")\n",
    "                              \n",
    "                        data = {\n",
    "                              \"causal_graph_type\": row['causal_graph_type'],\n",
    "                              \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                              \"given_info_type\": row['info_type'],\n",
    "                              \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                              \"target\": target,\n",
    "                              \"truth\": result_text\n",
    "                          }\n",
    "                        # Write data to file\n",
    "                        json.dump(data, out_file, indent=4)\n",
    "                        # Increment counter\n",
    "                        count += 1\n",
    "\n",
    "          # With observations, answer counterfactual questions.\n",
    "          else:\n",
    "            for obs_val in list(itertools.product(*[(0, 1)] * (n))):\n",
    "              evid_text = \"\"\n",
    "              evid = {}\n",
    "              for i, (k, v) in enumerate(zip(obs_var, obs_val)):\n",
    "                evid_text += row[f'{k}_{v}']\n",
    "                evid[k] = v\n",
    "                if i == len(obs_var) - 1:\n",
    "                    evid_text += '.'\n",
    "                else:\n",
    "                    evid_text += ', '\n",
    "              filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],  \n",
    "                                                                      X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                      rel1 = rel1) +' '+\\\n",
    "              row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                       X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                       rel1 = rel1, evidence = evid_text)\n",
    "\n",
    "              for index, question in dat_query.iterrows():\n",
    "                if row['causal_graph_type'] in question['causal_graph_type'] and row['info_type'] in question['info_type'] and question['CI_engine to get the answer'] == 'counterfactual':\n",
    "                  for (cou_var, cou_val, y) in zip(['X','Z','W'],[0,1],[0,1]):\n",
    "                    if cou_var not in evid or cou_val != evid[cou_var]:\n",
    "                      query = question['query_word_form'].format(Z_string = row['Z_string_name'], Y_string=row['Y_string_name'],\n",
    "                                                                X_string = row['X_string_name'], X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                cou_var = row[f'{cou_var}_string_name'], cou_val = row[f'{cou_var}_{cou_val}'], Y_int = row[f'Y_{y}'], cou_val_original = row[f'{cou_var}_{abs(1-cou_val)}'])\n",
    "                      \n",
    "                      # Input observations into the SCM\n",
    "                      for (var, val) in evid.items():\n",
    "                        ctx[var] = val\n",
    "\n",
    "                      # Input counterfactual by setting cou_var to cou_val\n",
    "                      ctx[cou_var] = bool(cou_val)\n",
    "                      result = int(ctx['Y'] == y)\n",
    "\n",
    "                      # True state of Y in counterfactual world\n",
    "                      y_truth = int(ctx['Y'])\n",
    "                      result_text = row[f'Y_{y_truth}']\n",
    "                      target = 'Yes' if result else 'No'\n",
    "                      \n",
    "\n",
    "                      ctx.clear()\n",
    "                \n",
    "                      if count > 0:\n",
    "                        out_file.write(\",\\n\")\n",
    "                            \n",
    "                      data = {\n",
    "                            \"causal_graph_type\": row['causal_graph_type'],\n",
    "                            \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                            \"given_info_type\": row['info_type'],\n",
    "                            \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                            \"target\": target,\n",
    "                            \"truth\": result_text\n",
    "                        }\n",
    "                      # Write data to file\n",
    "                      json.dump(data, out_file, indent=4)\n",
    "                      # Increment counter\n",
    "                      count += 1\n",
    "\n",
    "              \n",
    "            \n",
    "\n",
    "  out_file.write(\"\\n]\")\n",
    "out_file.close()"
   ],
   "metadata": {
    "id": "tND6NuLGUvf7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('prop_diamond.json', 'r') as f:\n",
    "  unique_entries = {}\n",
    "  duplicates = []\n",
    "  data = json.load(f)\n",
    "  print(f\"Total number of data points in the file: {len(data)}\")\n",
    "\n",
    "\n",
    "  for entry in data:\n",
    "      entry_hash = hash(json.dumps(entry, sort_keys=True))\n",
    "      if entry_hash in unique_entries:\n",
    "          duplicates.append(entry)\n",
    "          print(\"Duplicate Entry:\")\n",
    "          print(json.dumps(entry, indent=2))\n",
    "          print(\"Original Entry:\")\n",
    "          print(json.dumps(unique_entries[entry_hash], indent=2))\n",
    "          print(\"-------------------\")\n",
    "      else:\n",
    "          unique_entries[entry_hash] = entry\n",
    "\n",
    "  if duplicates:\n",
    "      print(f\"There are {len(duplicates)} duplicates in the file.\")\n",
    "      print(f\"Total number of data points in the file: {len(data)}\")\n",
    "      # Do something with the duplicates, such as printing them or removing them from the original data\n",
    "  else:\n",
    "      print(\"There are no duplicates in the file.\")\n",
    "\n",
    "out_file.close()"
   ],
   "metadata": {
    "id": "T81cFcf1hTOV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "86c84a9a-4061-4a5b-e822-19794a542066"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of data points in the file: 960\n",
      "There are no duplicates in the file.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "conf_nonid_df = dat[(dat['info_type'] == 'text_nonid') & (dat['causal_graph_type'] == 'confounder')]"
   ],
   "metadata": {
    "id": "0XAoKDNc_rLW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conf_nonid_df"
   ],
   "metadata": {
    "id": "B0BFxoDcA4F7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "outputId": "4157a2f3-7e9d-40f5-e7b3-dc1f2ea6769d"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   causal_graph_type        which_example         X_string_name  \\\n",
       "0         confounder     simpson_hospital   hospital assignment   \n",
       "5         confounder  simpson_kidneystone  treatment assignment   \n",
       "10        confounder         simpson_drug       drug assingment   \n",
       "\n",
       "                       X_0                    X_1      Z_string_name  \\\n",
       "0    treated at hospital A  treated at hospital B                age   \n",
       "5   receiving no treatment    receiving treatment  kidney stone size   \n",
       "10                 no drug                   drug             gender   \n",
       "\n",
       "                   Z_0                 Z_1 Y_string_name          Y_0  ...  \\\n",
       "0                young                 old      recovery  not recover  ...   \n",
       "5   small kidney stone  large kidney stone      recovery  not recover  ...   \n",
       "10            non-male                male      recovery  not recover  ...   \n",
       "\n",
       "       semantic_graph (by topological order)_x  \\\n",
       "0   \\nconf->cause, conf->effect, cause->effect   \n",
       "5   \\nconf->cause, conf->effect, cause->effect   \n",
       "10  \\nconf->cause, conf->effect, cause->effect   \n",
       "\n",
       "         variable_correspondence_x structural_equation  \\\n",
       "0   {X: cause, Y: effect, Z: conf}                       \n",
       "5   {X: cause, Y: effect, Z: conf}                       \n",
       "10  {X: cause, Y: effect, Z: conf}                       \n",
       "\n",
       "                             given_info_word_template  \\\n",
       "0   We know that {X_string} has an effect on {Y_st...   \n",
       "5   We know that {X_string} has an effect on {Y_st...   \n",
       "10  We know that {X_string} has an effect on {Y_st...   \n",
       "\n",
       "                                           extra_info  \\\n",
       "0   We observe that the probability of {Y_1} given...   \n",
       "5   We observe that the probability of {Y_1} given...   \n",
       "10  We observe that the probability of {Y_1} given...   \n",
       "\n",
       "                           suitable_CI_engines    \\\n",
       "0   do_calculus; counterfactual_formal_solver,     \n",
       "5   do_calculus; counterfactual_formal_solver,     \n",
       "10  do_calculus; counterfactual_formal_solver,     \n",
       "\n",
       "                       relation1 relation2   info_type  \n",
       "0   [greater than, smaller than]        []  text_nonid  \n",
       "5   [greater than, smaller than]        []  text_nonid  \n",
       "10  [greater than, smaller than]        []  text_nonid  \n",
       "\n",
       "[3 rows x 24 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-11a59030-4c2b-48d8-945d-71242ffaef21\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>causal_graph_type</th>\n",
       "      <th>which_example</th>\n",
       "      <th>X_string_name</th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>Z_string_name</th>\n",
       "      <th>Z_0</th>\n",
       "      <th>Z_1</th>\n",
       "      <th>Y_string_name</th>\n",
       "      <th>Y_0</th>\n",
       "      <th>...</th>\n",
       "      <th>semantic_graph (by topological order)_x</th>\n",
       "      <th>variable_correspondence_x</th>\n",
       "      <th>structural_equation</th>\n",
       "      <th>given_info_word_template</th>\n",
       "      <th>extra_info</th>\n",
       "      <th>suitable_CI_engines</th>\n",
       "      <th></th>\n",
       "      <th>relation1</th>\n",
       "      <th>relation2</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_hospital</td>\n",
       "      <td>hospital assignment</td>\n",
       "      <td>treated at hospital A</td>\n",
       "      <td>treated at hospital B</td>\n",
       "      <td>age</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[]</td>\n",
       "      <td>text_nonid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_kidneystone</td>\n",
       "      <td>treatment assignment</td>\n",
       "      <td>receiving no treatment</td>\n",
       "      <td>receiving treatment</td>\n",
       "      <td>kidney stone size</td>\n",
       "      <td>small kidney stone</td>\n",
       "      <td>large kidney stone</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[]</td>\n",
       "      <td>text_nonid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_drug</td>\n",
       "      <td>drug assingment</td>\n",
       "      <td>no drug</td>\n",
       "      <td>drug</td>\n",
       "      <td>gender</td>\n",
       "      <td>non-male</td>\n",
       "      <td>male</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[]</td>\n",
       "      <td>text_nonid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11a59030-4c2b-48d8-945d-71242ffaef21')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-11a59030-4c2b-48d8-945d-71242ffaef21 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-11a59030-4c2b-48d8-945d-71242ffaef21');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# confounder graph with insufficient text info\n",
    "with open('conf_nonid.json', 'w') as out_file:\n",
    "  out_file.write(\"[\\n\")\n",
    "  # Counter for number of entries written\n",
    "  count = 0\n",
    "  for idx, row in dat.iterrows():\n",
    "    if row['causal_graph_type'] == 'confounder' and row['info_type'] == 'text_nonid':\n",
    "      # loop over 2 different Structural equations\n",
    "      for rel1 in row['relation1']:    \n",
    "        filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                                X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1']) +' '+\\\n",
    "                          row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                  X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                  rel1 = rel1)\n",
    "        \n",
    "        # loop over all applicible queries with no observation (do_calculus only)\n",
    "        for index, question in dat_query.iterrows():\n",
    "          if 'confounder' in question['causal_graph_type'] and 'text_nonid' in question['info_type'] and question['CI_engine to get the answer'] == 'do_calculus': \n",
    "            if question['query_id'] == 'adj':\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                    X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                    W_0 = row['W_0'], W_1 = row['W_1'])\n",
    "\n",
    "              target = row['Z_string_name']\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "                    \n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "\n",
    "            else:\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                    X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                    W_0 = row['W_0'], W_1 = row['W_1'])\n",
    "\n",
    "              target = 'cannot determine'\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "                    \n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "            # Write data to file\n",
    "            json.dump(data, out_file, indent=4)\n",
    "            # Increment counter\n",
    "            count += 1\n",
    "          \n",
    "\n",
    "          elif 'confounder' in question['causal_graph_type'] and 'text_nonid' in question['info_type'] and question['CI_engine to get the answer'] == 'counterfactual': \n",
    "            # With observations, answer counterfactual questions.\n",
    "            s = {'X','Z','Y'}\n",
    "            powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "            powerset = [set(subset) for subset in powerset]\n",
    "            filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                              X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                              rel1 = rel1)+' '+\\\n",
    "                              row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                      X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                      rel1 = rel1)\n",
    "\n",
    "\n",
    "            for obs_set in powerset:\n",
    "              n = len(obs_set)\n",
    "              if n > 0:\n",
    "                for obs_val in list(itertools.product(*[(0, 1)] * (n))):\n",
    "                  evid_text = \"\"\n",
    "                  evid = {}\n",
    "                  for i, (k, v) in enumerate(zip(obs_set, obs_val)):\n",
    "                    evid_text += row[f'{k}_{v}']\n",
    "                    evid[k] = v\n",
    "                    if i != len(obs_set) - 1:\n",
    "                      evid_text += ', '\n",
    "                        \n",
    "                  for cou_val in [0,1]:\n",
    "                    if 'X' not in evid or cou_val != evid['X']:\n",
    "                      query = question['query_word_form'].format(X_string = row['X_string_name'],  Y_1 = row['Y_1'],\n",
    "                                                                X_cou_val = row[f'X_{cou_val}'], val_original = row[f'X_{abs(1-cou_val)}'], evidence = evid_text)\n",
    "            \n",
    "                      if count > 0:\n",
    "                        out_file.write(\",\\n\")\n",
    "\n",
    "                      target = 'cannot determine'\n",
    "                            \n",
    "                      data = {\n",
    "                            \"causal_graph_type\": row['causal_graph_type'],\n",
    "                            \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                            \"given_info_type\": row['info_type'],\n",
    "                            \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                            \"target\": target\n",
    "                        }\n",
    "                      # Write data to file\n",
    "                      json.dump(data, out_file, indent=4)\n",
    "                      # Increment counter\n",
    "                      count += 1             \n",
    "                  \n",
    "\n",
    "  out_file.write(\"\\n]\")\n",
    "out_file.close()\n"
   ],
   "metadata": {
    "id": "sqsI1Jr9MdEA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conf_id_df = dat[(dat['info_type'] == 'text') & (dat['causal_graph_type'] == 'confounder')]"
   ],
   "metadata": {
    "id": "Ro45D_jBOEoK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "conf_id_df"
   ],
   "metadata": {
    "id": "7ZEfBuxPOHnq",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "outputId": "d6b47698-63dd-475d-b15f-cff50aa7c1f5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   causal_graph_type        which_example         X_string_name  \\\n",
       "1         confounder     simpson_hospital   hospital assignment   \n",
       "6         confounder  simpson_kidneystone  treatment assignment   \n",
       "11        confounder         simpson_drug       drug assingment   \n",
       "\n",
       "                       X_0                    X_1      Z_string_name  \\\n",
       "1    treated at hospital A  treated at hospital B                age   \n",
       "6   receiving no treatment    receiving treatment  kidney stone size   \n",
       "11                 no drug                   drug             gender   \n",
       "\n",
       "                   Z_0                 Z_1 Y_string_name          Y_0  ...  \\\n",
       "1                young                 old      recovery  not recover  ...   \n",
       "6   small kidney stone  large kidney stone      recovery  not recover  ...   \n",
       "11            non-male                male      recovery  not recover  ...   \n",
       "\n",
       "       semantic_graph (by topological order)_x  \\\n",
       "1   \\nconf->cause, conf->effect, cause->effect   \n",
       "6   \\nconf->cause, conf->effect, cause->effect   \n",
       "11  \\nconf->cause, conf->effect, cause->effect   \n",
       "\n",
       "         variable_correspondence_x structural_equation  \\\n",
       "1   {X: cause, Y: effect, Z: conf}                       \n",
       "6   {X: cause, Y: effect, Z: conf}                       \n",
       "11  {X: cause, Y: effect, Z: conf}                       \n",
       "\n",
       "                             given_info_word_template  \\\n",
       "1   We know that {X_string} has an effect on {Y_st...   \n",
       "6   We know that {X_string} has an effect on {Y_st...   \n",
       "11  We know that {X_string} has an effect on {Y_st...   \n",
       "\n",
       "                                           extra_info  \\\n",
       "1   We observe that the probability of {Y_1} given...   \n",
       "6   We observe that the probability of {Y_1} given...   \n",
       "11  We observe that the probability of {Y_1} given...   \n",
       "\n",
       "                           suitable_CI_engines    \\\n",
       "1   do_calculus; counterfactual_formal_solver,     \n",
       "6   do_calculus; counterfactual_formal_solver,     \n",
       "11  do_calculus; counterfactual_formal_solver,     \n",
       "\n",
       "                       relation1                relation2 info_type  \n",
       "1   [greater than, smaller than]  [opposite of relation1]      text  \n",
       "6   [greater than, smaller than]  [opposite of relation1]      text  \n",
       "11  [greater than, smaller than]  [opposite of relation1]      text  \n",
       "\n",
       "[3 rows x 24 columns]"
      ],
      "text/html": [
       "\n",
       "  <div id=\"df-0c3d6468-967d-4d7d-a0e1-f6d4ccc9e892\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>causal_graph_type</th>\n",
       "      <th>which_example</th>\n",
       "      <th>X_string_name</th>\n",
       "      <th>X_0</th>\n",
       "      <th>X_1</th>\n",
       "      <th>Z_string_name</th>\n",
       "      <th>Z_0</th>\n",
       "      <th>Z_1</th>\n",
       "      <th>Y_string_name</th>\n",
       "      <th>Y_0</th>\n",
       "      <th>...</th>\n",
       "      <th>semantic_graph (by topological order)_x</th>\n",
       "      <th>variable_correspondence_x</th>\n",
       "      <th>structural_equation</th>\n",
       "      <th>given_info_word_template</th>\n",
       "      <th>extra_info</th>\n",
       "      <th>suitable_CI_engines</th>\n",
       "      <th></th>\n",
       "      <th>relation1</th>\n",
       "      <th>relation2</th>\n",
       "      <th>info_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_hospital</td>\n",
       "      <td>hospital assignment</td>\n",
       "      <td>treated at hospital A</td>\n",
       "      <td>treated at hospital B</td>\n",
       "      <td>age</td>\n",
       "      <td>young</td>\n",
       "      <td>old</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[opposite of relation1]</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_kidneystone</td>\n",
       "      <td>treatment assignment</td>\n",
       "      <td>receiving no treatment</td>\n",
       "      <td>receiving treatment</td>\n",
       "      <td>kidney stone size</td>\n",
       "      <td>small kidney stone</td>\n",
       "      <td>large kidney stone</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[opposite of relation1]</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>confounder</td>\n",
       "      <td>simpson_drug</td>\n",
       "      <td>drug assingment</td>\n",
       "      <td>no drug</td>\n",
       "      <td>drug</td>\n",
       "      <td>gender</td>\n",
       "      <td>non-male</td>\n",
       "      <td>male</td>\n",
       "      <td>recovery</td>\n",
       "      <td>not recover</td>\n",
       "      <td>...</td>\n",
       "      <td>\\nconf-&gt;cause, conf-&gt;effect, cause-&gt;effect</td>\n",
       "      <td>{X: cause, Y: effect, Z: conf}</td>\n",
       "      <td></td>\n",
       "      <td>We know that {X_string} has an effect on {Y_st...</td>\n",
       "      <td>We observe that the probability of {Y_1} given...</td>\n",
       "      <td>do_calculus; counterfactual_formal_solver,</td>\n",
       "      <td></td>\n",
       "      <td>[greater than, smaller than]</td>\n",
       "      <td>[opposite of relation1]</td>\n",
       "      <td>text</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c3d6468-967d-4d7d-a0e1-f6d4ccc9e892')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0c3d6468-967d-4d7d-a0e1-f6d4ccc9e892 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0c3d6468-967d-4d7d-a0e1-f6d4ccc9e892');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# confounder graph given sufficient text info \n",
    "\n",
    "with open('conf_id.json', 'w') as out_file:\n",
    "  out_file.write(\"[\\n\")\n",
    "  # Counter for number of entries written\n",
    "  count = 0\n",
    "  for idx, row in dat.iterrows():\n",
    "    if row['causal_graph_type'] == 'confounder' and row['info_type'] == 'text':\n",
    "      # loop over 2 different Structural equations\n",
    "      for rel1 in row['relation1']:    \n",
    "        if rel1 == 'greater than':\n",
    "          rel2 = 'smaller than'\n",
    "        else:\n",
    "          rel2 = 'greater than'\n",
    "\n",
    "        filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                                X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1']) +' '+\\\n",
    "                          row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                  X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                  rel1 = rel1, rel2 = rel2)\n",
    "        \n",
    "        # loop over all applicible queries with no observation (do_calculus only)\n",
    "        for index, question in dat_query.iterrows():\n",
    "          if 'confounder' in question['causal_graph_type'] and 'text' in question['info_type'] and question['CI_engine to get the answer'] == 'do_calculus': \n",
    "            if question['query_id'] == 'adj':\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                    X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                    W_0 = row['W_0'], W_1 = row['W_1'])\n",
    "\n",
    "              target = row['Z_string_name']\n",
    "             \n",
    "            elif question['query_id'] == 'ace_pos':\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], W_string=row['W_string_name'], \n",
    "                                                                    X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                                    W_0 = row['W_0'], W_1 = row['W_1'])\n",
    "              \n",
    "              if rel2 == 'greater than':\n",
    "                target = f\"Yes, {row['X_1']} would increase one's chance of {row['Y_1']}\"\n",
    "              else:\n",
    "                target = f\"No, {row['X_1']} would decrease one's chance of {row['Y_1']}\"\n",
    "              \n",
    "\n",
    "\n",
    "            if count > 0:\n",
    "              out_file.write(\",\\n\")\n",
    "                  \n",
    "            data = {\n",
    "                  \"causal_graph_type\": row['causal_graph_type'],\n",
    "                  \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                  \"given_info_type\": row['info_type'],\n",
    "                  \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                  \"target\": target\n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Write data to file\n",
    "            json.dump(data, out_file, indent=4)\n",
    "            # Increment counter\n",
    "            count += 1\n",
    "          \n",
    "\n",
    "          elif 'confounder' in question['causal_graph_type'] and 'text' in question['info_type'] and question['CI_engine to get the answer'] == 'counterfactual': \n",
    "            # With observations, answer counterfactual questions.\n",
    "            s = {'X','Z','Y'}\n",
    "            powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "            powerset = [set(subset) for subset in powerset]\n",
    "            filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                              X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                              rel1 = rel1)+' '+\\\n",
    "                              row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                      X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'], \n",
    "                                                      rel1 = rel1, rel2 = rel2)\n",
    "\n",
    "\n",
    "            for obs_set in powerset:\n",
    "              n = len(obs_set)\n",
    "              if n > 0:\n",
    "                for obs_val in list(itertools.product(*[(0, 1)] * (n))):\n",
    "\n",
    "                  evid_text = \"\"\n",
    "                  evid = {}\n",
    "                  for i, (k, v) in enumerate(zip(obs_set, obs_val)):\n",
    "                    evid_text += row[f'{k}_{v}']\n",
    "                    evid[k] = v\n",
    "                    if i != len(obs_set) - 1:\n",
    "                      evid_text += ', '\n",
    "\n",
    "                  for cou_val in [0,1]:\n",
    "                    if 'X' not in evid or cou_val != evid['X']:\n",
    "                      query = question['query_word_form'].format(X_string = row['X_string_name'],  Y_1 = row['Y_1'],\n",
    "                                                                X_cou_val = row[f'X_{cou_val}'], val_original = row[f'X_{abs(1-cou_val)}'], evidence = evid_text)\n",
    "                      \n",
    "            \n",
    "                      if (rel2 == 'greater than' and cou_val == 1) or (rel2 == 'smaller than' and cou_val == 0):\n",
    "                        target = f\"Yes, {row[f'X_{cou_val}']} would have increased one's chance of {row['Y_1']}\"\n",
    "                      elif (rel2 == 'greater than' and cou_val == 0) or (rel2 == 'smaller than' and cou_val == 1):\n",
    "                        target = f\"No, {row[f'X_{cou_val}']} would have decreased one's chance of {row['Y_1']}\"\n",
    "                            \n",
    "                      if count > 0:\n",
    "                        out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "                      data = {\n",
    "                            \"causal_graph_type\": row['causal_graph_type'],\n",
    "                            \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                            \"given_info_type\": row['info_type'],\n",
    "                            \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                            \"target\": target\n",
    "                        }\n",
    "                      # Write data to file\n",
    "                      json.dump(data, out_file, indent=4)\n",
    "                      # Increment counter\n",
    "                      count += 1             \n",
    "                    \n",
    "\n",
    "  out_file.write(\"\\n]\")\n",
    "out_file.close()\n"
   ],
   "metadata": {
    "id": "0H0GTkAVNxIW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('conf_id.json', 'r') as f:\n",
    "  unique_entries = {}\n",
    "  duplicates = []\n",
    "  data = json.load(f)\n",
    "  print(f\"Total number of data points in the file: {len(data)}\")\n",
    "\n",
    "\n",
    "  for entry in data:\n",
    "      entry_hash = hash(json.dumps(entry, sort_keys=True))\n",
    "      if entry_hash in unique_entries:\n",
    "          duplicates.append(entry)\n",
    "          print(\"Duplicate Entry:\")\n",
    "          print(json.dumps(entry, indent=2))\n",
    "          print(\"Original Entry:\")\n",
    "          print(json.dumps(unique_entries[entry_hash], indent=2))\n",
    "          print(\"-------------------\")\n",
    "      else:\n",
    "          unique_entries[entry_hash] = entry\n",
    "\n",
    "  if duplicates:\n",
    "      print(f\"There are {len(duplicates)} duplicates in the file.\")\n",
    "      print(f\"Total number of data points in the file: {len(data)}\")\n",
    "      # Do something with the duplicates, such as printing them or removing them from the original data\n",
    "  else:\n",
    "      print(\"There are no duplicates in the file.\")\n",
    "\n",
    "out_file.close()"
   ],
   "metadata": {
    "id": "MTRF0YQ4CmG_",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "919294fc-44ff-4eba-c9b1-e2ac4ba295c4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of data points in the file: 216\n",
      "There are no duplicates in the file.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Collider\n",
    "# Share the same graphs as mediator2 and fork. In addition, we have examples from 'collider_>' and colider_m2'\n",
    "\n",
    "with open('collider_>.json', 'w') as out_file:\n",
    "  out_file.write(\"[\\n\")\n",
    "  # Counter for number of entries written\n",
    "  count = 0\n",
    "  for idx, row in dat.iterrows():\n",
    "    if row['causal_graph_type'] == 'collider_>' and row['info_type'] in ['text', 'monotonic']:\n",
    "      filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                            X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1']) +' '+\\\n",
    "                      row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                              X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'])\n",
    "      \n",
    "      # loop over all applicible queries with no observation (do_calculus only)\n",
    "      for index, question in dat_query.iterrows():\n",
    "        if 'collider_>' in question['causal_graph_type'] and row['info_type'] in question['info_type']: \n",
    "          if question['query_id'] == 'att':\n",
    "            # With observations, answer counterfactual questions.\n",
    "            s = {'X','Z','Y'}\n",
    "            powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "            powerset = [set(subset) for subset in powerset]\n",
    "\n",
    "            for obs_set in powerset:\n",
    "              n = len(obs_set)\n",
    "              if 'X' in obs_set:\n",
    "                for obs_val in list(itertools.product(*[(0, 1)] * (n-1))):\n",
    "                  obs_val = (1,) + obs_val\n",
    "                  evid_text = \"\"\n",
    "                  evid = {}\n",
    "                  for i, (k, v) in enumerate(zip(obs_set, obs_val)):\n",
    "                    evid_text += row[f'{k}_{v}']\n",
    "                    evid[k] = v\n",
    "                    if i != len(obs_set) - 1:\n",
    "                      evid_text += ', '\n",
    "\n",
    "                  query = question['query_word_form'].format(X_string = row['X_string_name'],  Y_1 = row['Y_1'],\n",
    "                                                            X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "                  \n",
    "                  target = f\"Yes, their chance of {row['Y_1']} would have decreased if their {row['X_string_name']} had been {row['X_0']}\"\n",
    "\n",
    "                      \n",
    "                  if count > 0:\n",
    "                    out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "                  data = {\n",
    "                        \"causal_graph_type\": row['causal_graph_type'],\n",
    "                        \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                        \"given_info_type\": row['info_type'],\n",
    "                        \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                        \"target\": target\n",
    "                    }\n",
    "                  # Write data to file\n",
    "                  json.dump(data, out_file, indent=4)\n",
    "                  # Increment counter\n",
    "                  count += 1   \n",
    "\n",
    "          else:\n",
    "            if question['query_id'] == 'collider_bias':\n",
    "              for (val, val1) in zip([0,1],[0,1]):\n",
    "                val2 = abs(1-val1)\n",
    "                query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], \n",
    "                                                                      X_val1 = row[f'X_{val1}'], Y_val = row[f'Y_{val}'], Z_val2 = row[f'Z_{val2}'])\n",
    "\n",
    "                target = f\"No, {row['X_string_name']} is independent of {row['Z_string_name']}\"\n",
    "                if count > 0:\n",
    "                  out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "                data = {\n",
    "                      \"causal_graph_type\": row['causal_graph_type'],\n",
    "                      \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                      \"given_info_type\": row['info_type'],\n",
    "                      'query_id': question['query_id'],\n",
    "                      \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                      \"target\": target\n",
    "                  }\n",
    "                # Write data to file\n",
    "                json.dump(data, out_file, indent=4)\n",
    "                # Increment counter\n",
    "                count += 1            \n",
    "        \n",
    "            elif question['query_id'] in ['nde_pos']:\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'],  Z_string = row['Z_string_name'], Y_1 = row['Y_1'],\n",
    "                                            X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "                \n",
    "\n",
    "              target = f\"Yes, {row['X_1']} must have a positive direct effect on the chance of {row['Y_1']}\"\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    'query_id': question['query_id'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "              # Write data to file\n",
    "              json.dump(data, out_file, indent=4)\n",
    "              # Increment counter\n",
    "              count += 1   \n",
    "\n",
    "            elif question['query_id'] in ['nie_polarity']:\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'],  Z_string = row['Z_string_name'], Y_1 = row['Y_1'],\n",
    "                              X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "              if row['causal_graph_type'] == 'collider_>':\n",
    "                target = f\"No, {row['X_string_name']} is independent of {row['Z_string_name']}\"\n",
    "\n",
    "              else: \n",
    "                target = f\"Yes, {row['X_1']} must have a positive direct effect on the chance of {row['Y_1']}\"    \n",
    "\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    'query_id': question['query_id'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "              # Write data to file\n",
    "              json.dump(data, out_file, indent=4)\n",
    "              # Increment counter\n",
    "              count += 1\n",
    "\n",
    "\n",
    "    if row['causal_graph_type'] == 'mediator1' and row['info_type'] in ['text', 'monotonic']:\n",
    "      filled_template = row['given_info_word_template'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                                          X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1']) +' '+\\\n",
    "                    row['extra_info'].format(X_string=row['X_string_name'], Y_string=row['Y_string_name'], Z_string=row['Z_string_name'],\n",
    "                                            X_0 = row['X_0'], X_1 = row['X_1'], Y_0 = row['Y_0'], Y_1 = row['Y_1'], Z_0 = row['Z_0'], Z_1 = row['Z_1'])\n",
    "      \n",
    "      # loop over all applicible queries with no observation (do_calculus only)\n",
    "      for index, question in dat_query.iterrows():\n",
    "        if 'collider_>' in question['causal_graph_type'] and row['info_type'] in question['info_type']: \n",
    "          if question['query_id'] == 'att':\n",
    "            # With observations, answer counterfactual questions.\n",
    "            s = {'X','Z','Y'}\n",
    "            powerset = set(frozenset(subset) for subset in itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1)))\n",
    "            powerset = [set(subset) for subset in powerset]\n",
    "\n",
    "            for obs_set in powerset:\n",
    "              n = len(obs_set)\n",
    "              if 'X' in obs_set:\n",
    "                for obs_val in list(itertools.product(*[(0, 1)] * (n-1))):\n",
    "                  obs_val = (1,) + obs_val\n",
    "                  evid_text = \"\"\n",
    "                  evid = {}\n",
    "                  for i, (k, v) in enumerate(zip(obs_set, obs_val)):\n",
    "                    evid_text += row[f'{k}_{v}']\n",
    "                    evid[k] = v\n",
    "                    if i != len(obs_set) - 1:\n",
    "                      evid_text += ', '\n",
    "\n",
    "                  query = question['query_word_form'].format(X_string = row['X_string_name'],  Y_1 = row['Y_1'],\n",
    "                                                            X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "                  \n",
    "                  target = f\"Yes, their chance of {row['Y_1']} would have decreased if their {row['X_string_name']} had been {row['X_0']}\"\n",
    "\n",
    "                      \n",
    "                  if count > 0:\n",
    "                    out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "                  data = {\n",
    "                        \"causal_graph_type\": row['causal_graph_type'],\n",
    "                        \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                        \"given_info_type\": row['info_type'],\n",
    "                        \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                        \"target\": target\n",
    "                    }\n",
    "                  # Write data to file\n",
    "                  json.dump(data, out_file, indent=4)\n",
    "                  # Increment counter\n",
    "                  count += 1   \n",
    "\n",
    "          else:\n",
    "            if question['query_id'] == 'collider_bias':\n",
    "              for (val, val1) in zip([0,1],[0,1]):\n",
    "                val2 = abs(1-val1)\n",
    "                query = question['query_word_form'].format(X_string = row['X_string_name'], Z_string = row['Z_string_name'], Y_string=row['Y_string_name'], \n",
    "                                                                      X_val1 = row[f'X_{val1}'], Y_val = row[f'Y_{val}'], Z_val2 = row[f'Z_{val2}'])\n",
    "\n",
    "                target = f\"No, {row['X_string_name']} is independent of {row['Z_string_name']}\"\n",
    "                if count > 0:\n",
    "                  out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "                data = {\n",
    "                      \"causal_graph_type\": row['causal_graph_type'],\n",
    "                      \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                      \"given_info_type\": row['info_type'],\n",
    "                      'query_id': question['query_id'],\n",
    "                      \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                      \"target\": target\n",
    "                  }\n",
    "                # Write data to file\n",
    "                json.dump(data, out_file, indent=4)\n",
    "                # Increment counter\n",
    "                count += 1            \n",
    "        \n",
    "            elif question['query_id'] in ['nde_pos']:\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'],  Z_string = row['Z_string_name'], Y_1 = row['Y_1'],\n",
    "                                            X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "                \n",
    "\n",
    "              target = f\"Yes, {row['X_1']} must have a positive direct effect on the chance of {row['Y_1']}\"\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    'query_id': question['query_id'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "              # Write data to file\n",
    "              json.dump(data, out_file, indent=4)\n",
    "              # Increment counter\n",
    "              count += 1   \n",
    "\n",
    "            elif question['query_id'] in ['nie_polarity']:\n",
    "              query = question['query_word_form'].format(X_string = row['X_string_name'],  Z_string = row['Z_string_name'], Y_1 = row['Y_1'],\n",
    "                              X_0 = row['X_0'], evidence = evid_text, X_1 = row['X_1'])\n",
    "              if row['causal_graph_type'] == 'collider_>':\n",
    "                target = f\"No, {row['X_string_name']} is independent of {row['Z_string_name']}\"\n",
    "\n",
    "              else: \n",
    "                target = f\"Yes, {row['X_1']} must have a positive direct effect on the chance of {row['Y_1']}\"    \n",
    "\n",
    "              if count > 0:\n",
    "                out_file.write(\",\\n\")\n",
    "\n",
    "\n",
    "              data = {\n",
    "                    \"causal_graph_type\": row['causal_graph_type'],\n",
    "                    \"causal_inference_engine\": question['CI_engine to get the answer'],\n",
    "                    \"given_info_type\": row['info_type'],\n",
    "                    'query_id': question['query_id'],\n",
    "                    \"input\": filled_template + ' ' + query.replace('\\n', ' '),\n",
    "                    \"target\": target\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "              # Write data to file\n",
    "              json.dump(data, out_file, indent=4)\n",
    "              # Increment counter\n",
    "              count += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "  out_file.write(\"\\n]\")\n",
    "out_file.close()\n"
   ],
   "metadata": {
    "id": "vsYCKlrCCpCv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open('conf_id.json', 'r') as f:\n",
    "  unique_entries = {}\n",
    "  duplicates = []\n",
    "  data = json.load(f)\n",
    "  print(f\"Total number of data points in the file: {len(data)}\")\n",
    "\n",
    "\n",
    "  for entry in data:\n",
    "      entry_hash = hash(json.dumps(entry, sort_keys=True))\n",
    "      if entry_hash in unique_entries:\n",
    "          duplicates.append(entry)\n",
    "          print(\"Duplicate Entry:\")\n",
    "          print(json.dumps(entry, indent=2))\n",
    "          print(\"Original Entry:\")\n",
    "          print(json.dumps(unique_entries[entry_hash], indent=2))\n",
    "          print(\"-------------------\")\n",
    "      else:\n",
    "          unique_entries[entry_hash] = entry\n",
    "\n",
    "  if duplicates:\n",
    "      print(f\"There are {len(duplicates)} duplicates in the file.\")\n",
    "      print(f\"Total number of data points in the file: {len(data)}\")\n",
    "      # Do something with the duplicates, such as printing them or removing them from the original data\n",
    "  else:\n",
    "      print(\"There are no duplicates in the file.\")\n",
    "\n",
    "out_file.close()"
   ],
   "metadata": {
    "id": "QAwUtEZfv3IJ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "353f39e1-b354-4844-b555-f9c94ad6b3e4"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of data points in the file: 216\n",
      "There are no duplicates in the file.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3_LyWPej-w4P"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
